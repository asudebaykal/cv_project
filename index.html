<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Computer Vision Project</title>
    <link rel="stylesheet" type="text/css" href="./style.css">
  </head>

  <body>
    <div id="navContainer">
      <div id="btnContainers">
        <button class="btn" onclick="scrolls('abstract')">Abstract</button>
        <button class="btn" onclick="scrolls('problemStatement')">Problem Statement</button>
        <button class="btn" onclick="scrolls('approach')">Approach</button>
        <button class="btn" onclick="scrolls('intro')">Introduction</button>
        <button class="btn" onclick="scrolls('selection')">Selections</button>
        <button class="btn" onclick="scrolls('dataset')">Dataset</button>
        <button class="btn" onclick="scrolls('expRes')">Experiment & Results</button>
        <button class="btn" onclick="scrolls('qualRes')">Qualitative Results</button>
        <button class="btn" onclick="scrolls('conclusion')">Conclusion</button>
      </div>
    </div>

    <div id="mainSection">
      <div id="mainHeader">
        <img id="vtLogo" src="./assets/imgs/logo.png"/>
        <div id="mainSubheader">
          <text id="title">Computer Vision Project</text>
          <text id="authors">Xinrui Li, Asude Baykal, Kavin Chaisawangwong</text>
        </div>
      </div>
  
      <!-- <div id="blank"></div> -->
  
      <div class="alt" id="abstract">
        <text class="header" id="abstractHeader">Abstract</text>
        <text class="body" id="abstractBody">
          As self-driving vehicles or automotive driver assistance have become more widespread in real life, it is important to develop intelligent vehicle detection systems that will improve road safety and driving performance. However, vehicle detection systems encounter many challenges due to the number of vehicles in a traffic, different sized vehicles, and other environmental factors such as air clarity, weather conditions. Therefore, it is important to implement an automated vehicle detection system that can accurately locate a vehicle, and predict its coordinates. This paper implements and compares three methods of car detection using supervised learning with 2D images and their annotations.
        </text>
      </div>
  
      <!-- <div id="blank"></div> -->
  
      <div id="intro">
        <text class="header" id="introHeader">Previous Works</text>
        <text class="body" id="introBody">
          Vehicle detection systems using computer vision has been popular during the recent years. These systems can be used for road assistance for drivers and self-driving vehicles. According to vehicle accident statistics, one of the main threats is the other cars on the road [Zeehang]. It is important to develop vehicle detection systems to mitigate the road dangers through automotive vehicle assistance, but also improve the performance of self-driving vehicles. Unfortunately, understanding the traffic scene as accurately and fast as possible is a hard task due to the complex traffic conditions and air clarity. In this paper, we will be analyzing computer vision-based vehicle detection algorithms that use 2D images from the view of the camera mounted on the vehicle.
        </text>
      </div>
  
      <!-- <div id="blank"></div> -->
  
      <div class="alt" id="problemStatement">
        <text class="header" id="probStateIntro">Problem Statement</text>
        <text class="body" id="probStateBody">
          This project will provide a comparison of the results yielded by applying a 
          traditional and two different deep learning methods to detecting vehicles in 
          a static image. The methodology of each model will be categorically detailed along 
          with the results produced by their respective application.
        </text>
      </div>
  
      <!-- <div id="blank"></div> -->
  
      <div id="approach">
        <text class="header" id="apprIntro">Approach</text>
        <text class="body" id="apprBody">
          We used the self driving car dataset from roboflow [3] which contains over 30000 images, and we split them into 80% training set, 10% validation set, 10% test set. However, we found that the given true results for bounding boxes are not always correct. The results sometimes assign several different boxes to one car object which will lower our accuracy. So, before calculating the accuracy, we pre-process the results by deleting the boxes whose coordinate value is less than a certain threshold. We also found that our deep learning models will detect some car objects which are not in the result. We then decided to generate our prediction accuracy by only counting the detected boxes corresponding to the true results.
          Output bounding box coordinate of OpenCV method below is in terms of [x center, y center, width, height] of box. All other models’ output will be in terms of [x start, y start, x end, y end], where start is upper left corner, end is lower right corner. Interest of Union (IoU) will be calculated to measure the similarity between the predicted pixel coordinates of each vehicle and the ground truth pixel coordinates. When we get the bounding box vector from model output, we will calculate the rectangle area of both true result and prediction result. IoU can be calculated using formula below:          
        </text>
      </div>
  
      <!-- <div id="blank"></div> -->
  
      <div class="alt" id="selection">
        <text class="header" id="selectIntro">Selection of Models</text>
        <text id="selectBody">
          The architectures of the three models mentioned can be found below.
        </text>
        <ol id="selectList">
          <li>OpenCV</li>
          <text>
            Instead of using machine learning models, traditional image processing techniques, 
            like Canny Edge Detection, color thresholding and Harris Corner Detection, in 
            OpenCV will be used to directly detect multiple car objects in one image.
          </text>
          <li>CNNs</li>
            <ol type="a" id="subSelectList">
              <li>Simple CNN</li>
              <text>
                A simple Convolutional Neural Network consists of multiple convolutional layers 
                and pooling layers. A convolutional layer extracts the important features by 
                applying a filter repeatedly on each pixel resulting in a feature map. An 
                activation function such as ReLU will introduce non-linearity to the model. 
                The following pooling layers will be used to downsample each feature map using 
                averaging or selecting maximum value. 
              </text>
              <div class="imgContainer" id="simpleCnnSection">
                <img class="img" src="./assets/imgs/simplecnn.png"/>
              </div>
              <li>ResNet50</li>
              <text>
                A ResNet50 model contains 5 stages, each with a convolution and identity block. 
                Each respective convolution and identity block contains 3 additional convolution 
                layers. Hence, by convention, this model is a neural network that has 50 layers. 
                Skip connection distinguishes this model from its predecessors, where convolution 
                layers are not only stacked, but the original input is also added to the output 
                of the convolution block.
              </text>
              <div class="imgContainer" id="resnetSection">
                <img class="img" src="./assets/imgs/resnet.png"/>
              </div>
            </ol>
          <li>YoloV4</li>
          <text>
            YOLO v4 model contains CSPDarknet53 backbone for feature extraction, SPP additional 
            module for separating significant context features, PANet path-aggregation neck for 
            feature aggregation, and YOLOv3 head for locating bound boxes and performing 
            classification. (A. Bochkovskiy et al.)[7]
          </text>
          <div class="imgContainer" id="yolov4">
            <img class="img" src="./assets/imgs/yolo.png"/>
          </div>
        </ol> 
      </div>
  
      <!-- <div id="blank"></div> -->
  
      <div id="dataset">
        <text class="header" id="datasetIntro">Dataset</text>
        <text class="body" id="datasetBody">
          The dataset that will be used for the various models is the “Udacity Annotated Driving 
          Dataset” provided by Roboflow public archive. It includes driving in Mountain View 
          California and neighboring cities during daylight conditions, containing over 65,000 
          labels across 9,423 frames (Nelson)[9] and (Roboflow)[10].
        </text>
      </div>
  
      <!-- <div id="blank"></div> -->
  
      <div class="alt" id="expRes">
        <text class="header" id="expResIntro">Experiments and Results</text>
        <text class="body" id="expResBody"></text>
      </div>
      
      <!-- <div id="blank"></div> -->
  
      <div id="qualRes">
        <text class="header" id="qualResIntro">Qualitative Results</text>
        <text class="body" id="qualResIntro"></text>
      </div>
  
      <!-- <div id="blank"></div> -->
      
      <div class="alt" id="conclusion">
        <text class="header" id="concIntro">Conclusion</text>
        <text class="body" id="concBody"></text>
      </div>
  
      <!-- <div id="blank"></div> -->
      
      <div id="references">
        <text class="header" id="refIntro">References</text>
        <text class="body" id="refBody"></text>
      </div>
    </div>
    <script src="script.js"></script>
  </body>
</html>
